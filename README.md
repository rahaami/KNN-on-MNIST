# KNN-on-MNIST
Applying KNN classifier in order to explore MNIST dataset

The provided code implements a K-nearest neighbors (KNN) classifier for classifying handwritten digits in the MNIST dataset. So, it started by loading the dataset and extracting the features and labels, selecting randomly a subset of the training data to reduce the computational complexity, applying Principal Component Analysis (PCA) to reduce the dimensionality of the features and extracting the most informative features. The transformed features are then visualized using t-distributed Stochastic Neighbor Embedding (t-SNE), which maps the high-dimensional features into a 2D space for visualization purposes. The KNN classifier is then trained that reduced-dimension features as well. The distance metric used for computing distances between data points is the Euclidean distance. The classifier predicts the labels for both the training and testing datasets. Moreover, the code evaluates the accuracy of the classifier on the training dataset and saves the predictions for the testing dataset. Finally, it measures and prints the "wall-clock" time, indicating the total time taken for the execution of the code. Additionally, the code includes another example of using the KNN algorithm for digit classification using the load_digits dataset from scikit-learn. It applies PCA for dimensionality reduction and computes the confusion matrix for evaluating the performance. It also plots the accuracy as a function of the number of neighbors (K) for a specific digit.

In conclusion, the code demonstrates the implementation of a KNN classifier for digit classification, including preprocessing steps, dimensionality reduction, visualization, and performance evaluation.
